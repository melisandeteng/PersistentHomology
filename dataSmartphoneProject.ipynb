{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistent homology for smartphone data analysis (pedestrian recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Description__: The goal of this is project to illustrate, on a toy example, the benefit of “coordinate in- variance” of persistent homology. The walk of 3 pedestrians A, B and C, has been recorded using the accelerometer sensor of a smartphone carried in the their pocket, giving rise to 3 multivariate time series in R3: each time series represents the 3 coordinates of the acceleration of the corresponding pedestrian in a coordinate system attached to the sensor. As, the smartphone was carried in unknown different positions and was not fixed, these time series cannot be compared coordinates by coordinates. Using a sliding window, each series has been splitted in a list of 100 times series made of 200 consecutive points, that are stored in data A, data B and data C. To each set of 200 points is associated a label A, B or C stored in label (see the data set and the Python script to load the data). The objective is to compute the persistence diagrams of these 3D point clouds and use them to achieve a pedestrian recognition task (supervised setting).\n",
    "Note: This project requires some (basic) knowledge of learning (random forests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pickle\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path=\"/Users/Melisande/Gudhi/2017-10-02-10-19-30_GUDHI_2.0.1.tar/Gudhi/PersistentHomoligy/data_acc_rot.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "f = open(path,\"rb\")\n",
    "data = pickle.load(f,encoding='latin1')\n",
    "f.close()\n",
    "\n",
    "data_A = data[0]\n",
    "data_B = data[1] \n",
    "data_C = data[2]\n",
    "label = data[3]\n",
    "print(len(data[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and save the 0-dimensional and 1-dimensional persistence diagrams of the Rips filtrations (or alternately the alpha-shape filtrations) built on top of each of the 300 point clouds in R3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to_cython_gudhi='/Users/Melisande/Gudhi/2017-10-02-10-19-30_GUDHI_2.0.1.tar/Gudhi/build/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to_cython_gudhi='/Users/Melisande/Gudhi/2017-10-02-10-19-30_GUDHI_2.0.1.tar/Gudhi/build/'\n",
    "import sys\n",
    "sys.path.append(path_to_cython_gudhi)\n",
    "import gudhi\n",
    "#import gudhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'gudhi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e8b041c69491>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgudhi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'gudhi'"
     ]
    }
   ],
   "source": [
    "import gudhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About persistent homology \n",
    "\n",
    "### General definitions\n",
    "Let (P, D) be a metric space where P is a point set. \n",
    "Given r > 0, the Rips complex is the simplicial complex R(P) constituted by the simplexes such that$d(p,q) \\leq r$ for every pair of vertices in the simplex.  \n",
    "Constructing the Rips complex helps capture the topology of the data set. Choosing $r$ is a difficult task because if $r$ ois too small, the complex is a discrete set, and if $r$ is too large, the complex becomes a single high-dimensional complex.\n",
    "\n",
    "\n",
    "Given a filtration $(K_0, K_1, ..., K_n)$, the $p$-dimensional persistence diagram is the set of points (i,j) such that the number of $p$-dimensional homology classes born at $K_i$ that die entering $K_j$ is one.  \n",
    "Here the filtration will actually be a sequence of Rips complexes associated to the 3D point cloud for an increasing\n",
    "sequence of parameter values ($r_i$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0-dimensional persistence diagrams of the Rips filtrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to draw the 0-dimensional persistence diagram of the Rips filtrations, we use \\verb gudhi.RipsComplex on the 3D point cloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first put coordinates in point form\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-dimensional persistence diagrams of the Rips filtrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices of pairwise bottleneck distances between diagrams and use a dimensionality reduction algorithm to visualize them in 2D and 3D (e.g. Multidimensional Scaling).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bottleneck distance measures the similarity between two persistence diagrams. It is the shortest distance $d$ for which there exists a perfect matching between the points of the two diagrams such that any couple of matched points are at distance at most $d$. The cost of matching, i.e. taking a point $p$ of the first diagram to a point $p'$ of the second diagram corresponds the minimum between moving $p$ to $p'$ and moving both points on the diagonal.\n",
    "\n",
    "USE Gudhi::persistence_diagram::bottleneck_distance (const Persistence_diagram1 &diag1, const Persistence_diagram2 &diag2, double e=(std::numeric_limits< double >::min)())\n",
    "\n",
    "Why are we comparing 0 and 1-dimensional persistence diagrams of the Rips filtrations ? \n",
    "\n",
    "BLABLABLABLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE sklearn.manifold.MDS for multidimensional scaling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remarques du cours\n",
    "→ Vietoris-Rips (or Cech, witness) filtrations quickly become prohibitively large as\n",
    "the size of the data increases ( O(|X|\n",
    "d\n",
    ") ), making the computation of persistence\n",
    "practically almost impossible.\n",
    "→ Persistence diagrams of Rips-Vietoris (and Cˇech, witness,..) filtrations and\n",
    "Gromov-Hausdorff distance are very sensitive to noise and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing persistence landscape\n",
    "This function should take as input a persistence\n",
    "diagram dgm (in the Gudhi format), a dimension k, the endpoints xmin, xmax of an interval, the\n",
    "number nbnodes of nodes of a regular grid on the interval [xmin, xmax] and a number of landscapes\n",
    "nbld, and output a nbld × nbnodes array storing the values of the first nbld landscapes of dgm on the\n",
    "node of the grid. Check, on some simple examples that your code is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each 0-dimensional and 1-dimensional persistence diagrams, compute the first 5 landscapes on a\n",
    "relevant interval with a few hundred of nodes. Splitting randomly the data set into a 80/20 learning/test\n",
    "data, use a random forest to explore the performances of the 0-dimensional or 1-dimensional landscapes\n",
    "to classify pedestrians. An example of code to realize such an experiment can be downloaded at http:\n",
    "//geometrica.saclay.inria.fr/team/Fred.Chazal/Centrale2017.html. Compare the results you\n",
    "obtain using 0-dimensional landscapes, 1-dimensional landscapes or both.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same experiment as previously, but using the raw data ( 3 × 200 array of acceleration coordinates).\n",
    "Compare the obtained classification results to the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L1_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a08284169e96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mL_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL1_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mRF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mRF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'L1_list' is not defined"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Dec 17 18:33:31 2017\n",
    "\n",
    "@author: Fredreci Chazal - All rights reserved\n",
    "\"\"\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#### Classification with random forests \n",
    "#### Interesting compare with L0_list, L1_list and L_list\n",
    "#### where L0_list, L1_list and L_list are list storing the $0$-dimensional \n",
    "#### landscapes, $1$-dimensional landscapes, and the concatenation of both \n",
    "#### respectively\n",
    "avg = 0\n",
    "for i in range(20):\n",
    "    L_train, L_test, label_train, label_test = train_test_split(L1_list, label, test_size=0.2)\n",
    "    RF = RandomForestClassifier()\n",
    "    RF.fit(L_train, label_train)\n",
    "    print(np.mean(RF.predict(L_test) == label_test) )\n",
    "    avg += np.mean(RF.predict(L_test) == label_test)\n",
    "    #print(confusion_matrix(RF.predict(L_test), label_test))\n",
    "print (\"avg pred: \",avg/20)\n",
    "\n",
    "plt.plot(RF.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
